{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f68323-485d-42bc-b20a-1e863da925df",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "### Alex Ledgerwood\n",
    "\n",
    "Github Repo: https://github.com/ALedgerwood/Module-7-Final-Project\n",
    "Link to Tutorial:https://realpython.com/natural-language-processing-spacy-python/\n",
    "Link to Text to be analyzed:https://readingwise.com/assets/uploads/pdf/The_Hill_We_Climb_Transcript.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fee0dcd-9a69-49c8-84a9-527ea6ec090c",
   "metadata": {},
   "source": [
    "### import spacy and define nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "146cdae4-3b80-4423-9f7a-fd4b7f54cca9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1c7f7618e50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8cee52-b123-41fe-8b4e-1524a758f360",
   "metadata": {},
   "source": [
    "### call the .text attribute on each token to get the text contained within that token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8903c4fa-2f1b-4364-8e54-180d218ef19d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'tutorial',\n",
       " 'is',\n",
       " 'about',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'in',\n",
       " 'spaCy',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "introduction_doc = nlp(\"This tutorial is about Natural Language Processing in spaCy.\")\n",
    "type(introduction_doc)\n",
    "\n",
    "[token.text for token in introduction_doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db86815-d30e-4862-9ac5-0176a290f2ef",
   "metadata": {},
   "source": [
    "### do the same thing but reading from a file, not typing it in\n",
    "\n",
    "I don't know where the introduction.txt file is supposed to be - I am using the exact code from the tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e871e3c-17ff-4aae-ab62-3883b4b93d06",
   "metadata": {
    "tags": []
   },
   "source": [
    ">>> import pathlib\n",
    ">>> file_name = \"introduction.txt\"\n",
    ">>> introduction_doc = nlp(pathlib.Path(file_name).read_text(encoding=\"utf-8\"))\n",
    ">>> print ([token.text for token in introduction_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aa8f2b-1b66-4134-9f44-e469ccef55bb",
   "metadata": {},
   "source": [
    "### the .sents property is used to extract sentences from the Doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e88a2b3-1658-4182-9a1d-0206910efc7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus Proto is a Python...\n",
      "He is interested in learning...\n"
     ]
    }
   ],
   "source": [
    ">>> about_text = (\n",
    "...     \"Gus Proto is a Python developer currently\"\n",
    "...     \" working for a London-based Fintech\"\n",
    "...     \" company. He is interested in learning\"\n",
    "...     \" Natural Language Processing.\"\n",
    "... )\n",
    ">>> about_doc = nlp(about_text)\n",
    ">>> sentences = list(about_doc.sents)\n",
    ">>> len(sentences)\n",
    "2\n",
    ">>> for sentence in sentences:\n",
    "...     print(f\"{sentence[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef10447-734c-453a-8db6-d1157ba0364f",
   "metadata": {},
   "source": [
    "### Creating custom delimeters in sentence detection\n",
    "\n",
    "this uses elipses as a delimeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7340b72f-580d-48fb-87c9-08366328f1a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus, can you, ...\n",
      "never mind, I forgot what I was saying.\n",
      "So, do you think we should ...\n"
     ]
    }
   ],
   "source": [
    ">>> ellipsis_text = (\n",
    "...     \"Gus, can you, ... never mind, I forgot\"\n",
    "...     \" what I was saying. So, do you think\"\n",
    "...     \" we should ...\"\n",
    "... )\n",
    "\n",
    ">>> from spacy.language import Language\n",
    ">>> @Language.component(\"set_custom_boundaries\")\n",
    "... def set_custom_boundaries(doc):\n",
    "...     \"\"\"Add support to use `...` as a delimiter for sentence detection\"\"\"\n",
    "...     for token in doc[:-1]:\n",
    "...         if token.text == \"...\":\n",
    "...             doc[token.i + 1].is_sent_start = True\n",
    "...     return doc\n",
    "...\n",
    "\n",
    ">>> custom_nlp = spacy.load(\"en_core_web_sm\")\n",
    ">>> custom_nlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")\n",
    ">>> custom_ellipsis_doc = custom_nlp(ellipsis_text)\n",
    ">>> custom_ellipsis_sentences = list(custom_ellipsis_doc.sents)\n",
    ">>> for sentence in custom_ellipsis_sentences:\n",
    "...     print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5310147-5f15-4f76-add0-3ae00df97ee3",
   "metadata": {},
   "source": [
    "### Tokenization - showing that the token’s original index position in the string is still available as an attribute on Token\n",
    "\n",
    "could be useful for in-place word replacement down the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a1ee85d-68a4-410b-bade-b183c4ec8da0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus 0\n",
      "Proto 4\n",
      "is 10\n",
      "a 13\n",
      "Python 15\n",
      "developer 22\n",
      "currently 32\n",
      "working 42\n",
      "for 50\n",
      "a 54\n",
      "London 56\n",
      "- 62\n",
      "based 63\n",
      "Fintech 69\n",
      "company 77\n",
      ". 84\n",
      "He 86\n",
      "is 89\n",
      "interested 92\n",
      "in 103\n",
      "learning 106\n",
      "Natural 115\n",
      "Language 123\n",
      "Processing 132\n",
      ". 142\n"
     ]
    }
   ],
   "source": [
    ">>> import spacy\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    ">>> about_text = (\n",
    "...     \"Gus Proto is a Python developer currently\"\n",
    "...     \" working for a London-based Fintech\"\n",
    "...     \" company. He is interested in learning\"\n",
    "...     \" Natural Language Processing.\"\n",
    "... )\n",
    ">>> about_doc = nlp(about_text)\n",
    "\n",
    ">>> for token in about_doc:\n",
    "...     print (token, token.idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c86951c-6869-476e-b08b-6ae5e8a6f3dc",
   "metadata": {},
   "source": [
    "### Other attributes for the token class\n",
    "use f-string formatting to output a table accessing some common attributes from each Token in Doc:\n",
    "\n",
    ".text_with_ws prints the token text along with any trailing space, if present.\n",
    "\n",
    ".is_alpha indicates whether the token consists of alphabetic characters or not.\n",
    "\n",
    ".is_punct indicates whether the token is a punctuation symbol or not.\n",
    "\n",
    ".is_stop indicates whether the token is a stop word or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6ec7791-a562-494e-8a4a-b0205fe76fab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Whitespace  Is Alphanumeric?Is Punctuation?   Is Stop Word?\n",
      "Gus                   True           False             False\n",
      "Proto                 True           False             False\n",
      "is                    True           False             True\n",
      "a                     True           False             True\n",
      "Python                True           False             False\n",
      "developer             True           False             False\n",
      "currently             True           False             False\n",
      "working               True           False             False\n",
      "for                   True           False             True\n",
      "a                     True           False             True\n",
      "London                True           False             False\n",
      "-                     False          True              False\n",
      "based                 True           False             False\n",
      "Fintech               True           False             False\n",
      "company               True           False             False\n",
      ".                     False          True              False\n",
      "He                    True           False             True\n",
      "is                    True           False             True\n",
      "interested            True           False             False\n",
      "in                    True           False             True\n",
      "learning              True           False             False\n",
      "Natural               True           False             False\n",
      "Language              True           False             False\n",
      "Processing            True           False             False\n",
      ".                     False          True              False\n"
     ]
    }
   ],
   "source": [
    ">>> print(\n",
    "...     f\"{'Text with Whitespace':22}\"\n",
    "...     f\"{'Is Alphanumeric?':15}\"\n",
    "...     f\"{'Is Punctuation?':18}\"\n",
    "...     f\"{'Is Stop Word?'}\"\n",
    "... )\n",
    ">>> for token in about_doc:\n",
    "...     print(\n",
    "...         f\"{str(token.text_with_ws):22}\"\n",
    "...         f\"{str(token.is_alpha):15}\"\n",
    "...         f\"{str(token.is_punct):18}\"\n",
    "...         f\"{str(token.is_stop)}\"\n",
    "...     )\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72658f84-c377-4689-9e52-e94f2713653c",
   "metadata": {},
   "source": [
    "### custom tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f67bb31a-b169-41a7-a4c7-941a5ad0d6af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'a', 'London@based', 'Fintech', 'company', '.', 'He']\n"
     ]
    }
   ],
   "source": [
    ">>> custom_about_text = (\n",
    "...     \"Gus Proto is a Python developer currently\"\n",
    "...     \" working for a London@based Fintech\"\n",
    "...     \" company. He is interested in learning\"\n",
    "...     \" Natural Language Processing.\"\n",
    "... )\n",
    "\n",
    ">>> print([token.text for token in nlp(custom_about_text)[8:15]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507e9ff8-6ac4-4ebc-82a7-4fe4fe028bd9",
   "metadata": {},
   "source": [
    "### To include the @ symbol as a custom infix, you need to build your own Tokenizer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fd08a0d-6791-49ba-b691-72d8a894a754",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'a', 'London', '@', 'based', 'Fintech', 'company']\n"
     ]
    }
   ],
   "source": [
    ">>> import re\n",
    ">>> from spacy.tokenizer import Tokenizer\n",
    "\n",
    ">>> custom_nlp = spacy.load(\"en_core_web_sm\")\n",
    ">>> prefix_re = spacy.util.compile_prefix_regex(\n",
    "...     custom_nlp.Defaults.prefixes\n",
    "... )\n",
    ">>> suffix_re = spacy.util.compile_suffix_regex(\n",
    "...     custom_nlp.Defaults.suffixes\n",
    "... )\n",
    "\n",
    ">>> custom_infixes = [r\"@\"]\n",
    "\n",
    ">>> infix_re = spacy.util.compile_infix_regex(\n",
    "...     list(custom_nlp.Defaults.infixes) + custom_infixes\n",
    "... )\n",
    "\n",
    ">>> custom_nlp.tokenizer = Tokenizer(\n",
    "...     nlp.vocab,\n",
    "...     prefix_search=prefix_re.search,\n",
    "...     suffix_search=suffix_re.search,\n",
    "...     infix_finditer=infix_re.finditer,\n",
    "...     token_match=None,\n",
    "... )\n",
    "\n",
    ">>> custom_tokenizer_about_doc = custom_nlp(custom_about_text)\n",
    "\n",
    ">>> print([token.text for token in custom_tokenizer_about_doc[8:15]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c4a09a-f303-45ae-83de-ed04527616c5",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "Stop words are typically defined as the most common words in a language.\n",
    "\n",
    "With NLP, stop words are generally removed because they aren’t significant, and they heavily distort any word frequency analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87b68806-b0f9-4d18-b19a-085832895f75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "him\n",
      "seem\n",
      "had\n",
      "then\n",
      "should\n",
      "move\n",
      "’m\n",
      "becoming\n",
      "yours\n",
      "every\n"
     ]
    }
   ],
   "source": [
    ">>> import spacy\n",
    ">>> spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    ">>> len(spacy_stopwords)\n",
    "326\n",
    ">>> for stop_word in list(spacy_stopwords)[:10]:\n",
    "...     print(stop_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d14ca9-db2d-455e-86cd-a1f426246d90",
   "metadata": {},
   "source": [
    "### .is_stop attribute\n",
    "You don’t need to access this list directly, though. You can REMOVE STOP WORDS from the input text by making use of the .is_stop attribute of each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac00cb07-822f-4a7c-aa91-8bd2475b6b92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gus, Proto, Python, developer, currently, working, London, -, based, Fintech, company, ., interested, learning, Natural, Language, Processing, .]\n"
     ]
    }
   ],
   "source": [
    ">>> custom_about_text = (\n",
    "...     \"Gus Proto is a Python developer currently\"\n",
    "...     \" working for a London-based Fintech\"\n",
    "...     \" company. He is interested in learning\"\n",
    "...     \" Natural Language Processing.\"\n",
    "... )\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    ">>> about_doc = nlp(custom_about_text)\n",
    ">>> print([token for token in about_doc if not token.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d8ac64-f5f0-484e-87c3-7551a8fa5315",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "a root word, is called a lemma.\n",
    "\n",
    "reduce the inflected forms of a word so that they can be analyzed as a single item. It can also help you normalize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc87a1d5-9487-4e0d-9b88-28d81d575a94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  is : be\n",
      "                  He : he\n",
      "               keeps : keep\n",
      "          organizing : organize\n",
      "             meetups : meetup\n",
      "               talks : talk\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    ">>> conference_help_text = (\n",
    "...     \"Gus is helping organize a developer\"\n",
    "...     \" conference on Applications of Natural Language\"\n",
    "...     \" Processing. He keeps organizing local Python meetups\"\n",
    "...     \" and several internal talks at his workplace.\"\n",
    "... )\n",
    ">>> conference_help_doc = nlp(conference_help_text)\n",
    ">>> for token in conference_help_doc:\n",
    "...     if str(token) != str(token.lemma_):\n",
    "...         print(f\"{str(token):>20} : {str(token.lemma_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453bfdc7-a3b8-4ae0-92fa-cbfcc48bf70c",
   "metadata": {},
   "source": [
    "### Word Frequency\n",
    "once you've lemmatized (AND removed stop words), you can perform statitstical analysis on text\n",
    "\n",
    "the following code tells the most common words, so you can assume they are what the text is about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12746896-600e-4954-8d37-e0299e4e2a78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Gus', 4), ('London', 3), ('Natural', 3), ('Language', 3), ('Processing', 3)]\n"
     ]
    }
   ],
   "source": [
    ">>> import spacy\n",
    ">>> from collections import Counter\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    ">>> complete_text = (\n",
    "...     \"Gus Proto is a Python developer currently\"\n",
    "...     \" working for a London-based Fintech company. He is\"\n",
    "...     \" interested in learning Natural Language Processing.\"\n",
    "...     \" There is a developer conference happening on 21 July\"\n",
    "...     ' 2019 in London. It is titled \"Applications of Natural'\n",
    "...     ' Language Processing\". There is a helpline number'\n",
    "...     \" available at +44-1234567891. Gus is helping organize it.\"\n",
    "...     \" He keeps organizing local Python meetups and several\"\n",
    "...     \" internal talks at his workplace. Gus is also presenting\"\n",
    "...     ' a talk. The talk will introduce the reader about \"Use'\n",
    "...     ' cases of Natural Language Processing in Fintech\".'\n",
    "...     \" Apart from his work, he is very passionate about music.\"\n",
    "...     \" Gus is learning to play the Piano. He has enrolled\"\n",
    "...     \" himself in the weekend batch of Great Piano Academy.\"\n",
    "...     \" Great Piano Academy is situated in Mayfair or the City\"\n",
    "...     \" of London and has world-class piano instructors.\"\n",
    "... )\n",
    ">>> complete_doc = nlp(complete_text)\n",
    "\n",
    ">>> words = [\n",
    "...     token.text\n",
    "...     for token in complete_doc\n",
    "...     if not token.is_stop and not token.is_punct\n",
    "... ]\n",
    "\n",
    ">>> print(Counter(words).most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcfe823-2454-4add-9c10-f320339596a4",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5620d976-0f7e-42a5-af17-5d908775fbdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOKEN: Gus\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: Proto\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: is\n",
      "=====\n",
      "TAG: VBZ        POS: AUX\n",
      "EXPLANATION: verb, 3rd person singular present\n",
      "\n",
      "TOKEN: a\n",
      "=====\n",
      "TAG: DT         POS: DET\n",
      "EXPLANATION: determiner\n",
      "\n",
      "TOKEN: Python\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: developer\n",
      "=====\n",
      "TAG: NN         POS: NOUN\n",
      "EXPLANATION: noun, singular or mass\n",
      "\n",
      "TOKEN: currently\n",
      "=====\n",
      "TAG: RB         POS: ADV\n",
      "EXPLANATION: adverb\n",
      "\n",
      "TOKEN: working\n",
      "=====\n",
      "TAG: VBG        POS: VERB\n",
      "EXPLANATION: verb, gerund or present participle\n",
      "\n",
      "TOKEN: for\n",
      "=====\n",
      "TAG: IN         POS: ADP\n",
      "EXPLANATION: conjunction, subordinating or preposition\n",
      "\n",
      "TOKEN: a\n",
      "=====\n",
      "TAG: DT         POS: DET\n",
      "EXPLANATION: determiner\n",
      "\n",
      "TOKEN: London\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: -\n",
      "=====\n",
      "TAG: HYPH       POS: PUNCT\n",
      "EXPLANATION: punctuation mark, hyphen\n",
      "\n",
      "TOKEN: based\n",
      "=====\n",
      "TAG: VBN        POS: VERB\n",
      "EXPLANATION: verb, past participle\n",
      "\n",
      "TOKEN: Fintech\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: company\n",
      "=====\n",
      "TAG: NN         POS: NOUN\n",
      "EXPLANATION: noun, singular or mass\n",
      "\n",
      "TOKEN: .\n",
      "=====\n",
      "TAG: .          POS: PUNCT\n",
      "EXPLANATION: punctuation mark, sentence closer\n",
      "\n",
      "TOKEN: He\n",
      "=====\n",
      "TAG: PRP        POS: PRON\n",
      "EXPLANATION: pronoun, personal\n",
      "\n",
      "TOKEN: is\n",
      "=====\n",
      "TAG: VBZ        POS: AUX\n",
      "EXPLANATION: verb, 3rd person singular present\n",
      "\n",
      "TOKEN: interested\n",
      "=====\n",
      "TAG: JJ         POS: ADJ\n",
      "EXPLANATION: adjective (English), other noun-modifier (Chinese)\n",
      "\n",
      "TOKEN: in\n",
      "=====\n",
      "TAG: IN         POS: ADP\n",
      "EXPLANATION: conjunction, subordinating or preposition\n",
      "\n",
      "TOKEN: learning\n",
      "=====\n",
      "TAG: VBG        POS: VERB\n",
      "EXPLANATION: verb, gerund or present participle\n",
      "\n",
      "TOKEN: Natural\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: Language\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: Processing\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: .\n",
      "=====\n",
      "TAG: .          POS: PUNCT\n",
      "EXPLANATION: punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    ">>> import spacy\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    ">>> about_text = (\n",
    "...     \"Gus Proto is a Python developer currently\"\n",
    "...     \" working for a London-based Fintech\"\n",
    "...     \" company. He is interested in learning\"\n",
    "...     \" Natural Language Processing.\"\n",
    "... )\n",
    ">>> about_doc = nlp(about_text)\n",
    ">>> for token in about_doc:\n",
    "...     print(\n",
    "...         f\"\"\"\n",
    "... TOKEN: {str(token)}\n",
    "... =====\n",
    "... TAG: {str(token.tag_):10} POS: {token.pos_}\n",
    "... EXPLANATION: {spacy.explain(token.tag_)}\"\"\"\n",
    "...     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ebb28b-7fdd-43b0-a02b-8a0f46b8d8b7",
   "metadata": {},
   "source": [
    "### Pullout words by part of speech/category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a8f4486-6690-4b62-a0e1-8bde38d09187",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[developer, company]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> nouns = []\n",
    ">>> adjectives = []\n",
    ">>> for token in about_doc:\n",
    "...     if token.pos_ == \"NOUN\":\n",
    "...         nouns.append(token)\n",
    "...     if token.pos_ == \"ADJ\":\n",
    "...         adjectives.append(token)\n",
    "...\n",
    "\n",
    ">>> nouns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e72636-fb53-4d71-9135-99151cd937d1",
   "metadata": {},
   "source": [
    "### Visualization using spaCy's builtin called displaCy\n",
    "\n",
    "each token is assigned a POS tag written just below the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ced78fe-5718-4b1b-bacb-4a5ba4e11148",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    ">>> import spacy\n",
    ">>> from spacy import displacy\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    ">>> about_interest_text = (\n",
    "...     \"He is interested in learning Natural Language Processing.\"\n",
    "... )\n",
    ">>> displacy.render(about_interest_doc, style=\"dep\", jupyter=True)\n",
    ">>> about_interest_doc = nlp(about_interest_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e885e-b35c-4426-bdaf-0042864704d1",
   "metadata": {},
   "source": [
    "### Creating Preprocessing Functions\n",
    "To bring your text into a format ideal for analysis, you can write preprocessing functions to encapsulate your cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1c576f-b1bd-4766-8721-a10c6f6d86ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    ">>> import spacy\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    ">>> complete_text = (\n",
    "...     \"Gus Proto is a Python developer currently\"\n",
    "...     \" working for a London-based Fintech company. He is\"\n",
    "...     \" interested in learning Natural Language Processing.\"\n",
    "...     \" There is a developer conference happening on 21 July\"\n",
    "...     ' 2019 in London. It is titled \"Applications of Natural'\n",
    "...     ' Language Processing\". There is a helpline number'\n",
    "...     \" available at +44-1234567891. Gus is helping organize it.\"\n",
    "...     \" He keeps organizing local Python meetups and several\"\n",
    "...     \" internal talks at his workplace. Gus is also presenting\"\n",
    "...     ' a talk. The talk will introduce the reader about \"Use'\n",
    "...     ' cases of Natural Language Processing in Fintech\".'\n",
    "...     \" Apart from his work, he is very passionate about music.\"\n",
    "...     \" Gus is learning to play the Piano. He has enrolled\"\n",
    "...     \" himself in the weekend batch of Great Piano Academy.\"\n",
    "...     \" Great Piano Academy is situated in Mayfair or the City\"\n",
    "...     \" of London and has world-class piano instructors.\"\n",
    "... )\n",
    ">>> complete_doc = nlp(complete_text)\n",
    ">>> def is_token_allowed(token):\n",
    "...     return bool(\n",
    "...         token\n",
    "...         and str(token).strip()\n",
    "...         and not token.is_stop\n",
    "...         and not token.is_punct\n",
    "...     )\n",
    "...\n",
    ">>> def preprocess_token(token):\n",
    "...     return token.lemma_.strip().lower()\n",
    "...\n",
    ">>> complete_filtered_tokens = [\n",
    "...     preprocess_token(token)\n",
    "...     for token in complete_doc\n",
    "...     if is_token_allowed(token)\n",
    "... ]\n",
    "\n",
    ">>> complete_filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec8c80c-0c27-4e76-9425-c88cf0afda11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
